{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão Linear\n",
    "\n",
    "Vamos supor que tenhamos dados em tabela sobre duas variáveis: **x** e **y**. Se colocarmos cada par **(x,y)** em um gráfico, teremos uma figura como a seguinte:\n",
    "\n",
    "![1](https://matheusfacure.github.io/img/tutorial/regr_lin_mqo/scatter.png)\n",
    "\n",
    "O que o algoritmo de regressão linear faz é simplesmente achar a reta que melhor se encaixa entre os pontos:\n",
    "\n",
    "Assim, podemos prever (com erro) um valor de y dado um valor de x. Por exemplo, nós não temos uma observação em que x=1, mas gostaríamos de prever qual seria o valor de y caso x fosse 1. Basta então olhar na linha qual valor de y quando x assume o valor 1. Na imagem acima, y seria aproximadamente 2.5 (ponto amarelo).\n",
    "\n",
    "![2](https://matheusfacure.github.io/img/tutorial/regr_lin_mqo/scatter_line.png)\n",
    "\n",
    "Ok. Esse exemplo é bem simples e meramente ilustrativo. Suponha agora que y não dependa mais apenas de x, mas de x e z. Bem, nesse caso, teríamos um gráfico em 3D e a regressão linear acharia o plano que melhor se encaixa nos dados. E para mais dimensões? Digamos que y dependa de 100 variáveis. Nós não podemos mais visualizar esse caso, mas sabemos que não é muito diferente dos casos 2D ou 3D, só que agora a regressão linear acha o hiperplano que melhor se encaixa nos dados. Se isso está um pouco abstrato e difícil de visualizar, pense  sempre em 3D quando trabalhando com muitas dimensões. É um truque muito útil que eu aprendi em um vídeo do professor de Geoffrey Hinton e, segundo ele, todo mundo faz isso: quando tentar visualizar 100 dimensões, por exemplo, pense em 3D e grite para si mesmo \"100D\" e você conseguirá abstrair grandes dimensionalidades.\n",
    "\n",
    "### Justificativa matemática\n",
    "Imagine que temos dados em tabela, sendo que cada linha é uma observação e cada coluna uma variável. Então escolhemos uma das colunas para ser nossa variável dependente y (aquela que queremos prever) e as outras serão as variáveis independentes (X). Nosso objetivo é aprender como chegar das variáveis independentes na variável dependente, ou, em outras palavras, prever y a partir de X. Note que X é uma matriz nxd, em que n é o número de observações e d o número de dimensões; y é um vetor coluna nx1. Podemos definir o problema como um sistema de equações em que cada equação é uma observação:\n",
    "\n",
    "$\\begin{cases}\n",
    "w_0 + w_1 x_1 + ... + w_d x_1 = y_1 \\\\\n",
    "w_0 + w_1 x_2 + ... + w_d x_2 = y_2 \\\\\n",
    "... \\\\\n",
    "w_0 + w_1 x_n + ... + w_d x_n = y_n \\\\\n",
    "\\end{cases}$\n",
    "\n",
    "Normalmente $n > d$, isto é, temos mais observações que dimensões. Sistemas assim costumam não ter solução, pois há muitas equações e poucas variáveis para ajustar. Intuitivamente, pense que, na prática, muitas coisas afetam a variável y, principalmente se ela for algo de interesse das ciências humanas como, por exemplo, preço, desemprego, felicidade etc. Muitas das coisas que afetam y não podem ser coletadas como dados; desse modo, as equações acima não teriam solução porque não teríamos todos os fatores que afetam y.\n",
    "\n",
    "Para lidar com esse problema, vamos adicionar nas equações um termo erro ε que representará os fatores que não conseguimos observar, erros de medição, etc.\n",
    "\n",
    "$\\begin{cases}\n",
    "w_0 + w_1 x_{11} + ... + w_d x_{1d} + \\varepsilon_1 = y_1 \\\\\n",
    "w_0 + w_1 x_{21} + ... + w_d x_{2d} + \\varepsilon_2 = y_2 \\\\\n",
    "... \\\\\n",
    "w_0 + w_1 x_{n1} + ... + w_d x_{nd} + \\varepsilon_3 = y_n \\\\\n",
    "\\end{cases}$\n",
    "\n",
    "Ou, em forma de matriz:\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "1 & x_{11} & ... & x_{1d} \\\\\n",
    "1 & x_{21} & ... & x_{2d} \\\\\n",
    "\\vdots & \\vdots& \\vdots & \\vdots \\\\\n",
    "1 & x_{n1} & ... & x_{nd} \\\\\n",
    "\\end{bmatrix}\n",
    "\\times\n",
    "\\begin{bmatrix}\n",
    "w_0 \\\\\n",
    "w_1 \\\\\n",
    "\\vdots \\\\\n",
    "w_d \\\\\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "\\varepsilon_0 \\\\\n",
    "\\varepsilon_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\varepsilon_n \\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "y_0 \\\\\n",
    "y_1 \\\\\n",
    "\\vdots \\\\\n",
    "y_n \\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$X_{nd} \\pmb{w}_{d1} + \\pmb{\\epsilon}_{n1} = \\pmb{y}_{n1}$\n",
    "\n",
    "Para estimar a equação acima, usaremos a técnica de Mínimos Quadrados Ordinários (MQO): queremos achar os $\\pmb{\\hat{w}}$  que minimizam os $n \\varepsilon^2$, ou, na forma de vetor, $\\pmb{\\epsilon}^T \\pmb{\\epsilon}$. Por que minimizar os erros quadrados? Assim como todo algoritmo de Aprendizado de Máquina, regressão linear também pode ser encarada como problemas de minimização de função custo. Então, nesse caso, nossa função custo é $L = \\pmb{\\epsilon}^T \\pmb{\\epsilon}$. Um nome comum dessa função é o custo quadrático $L2$, pois nesse caso o custo é o quadrado da norma $L2$ do vetor $\\pmb{\\epsilon}$. Note que nós poderíamos usar também a norma $L1$ do mesmo vetor como função custo. Ou ainda, poderíamos usar outras funções que adicionam uma penalidade também para o tamanho de $\\pmb{\\hat{w}}$, como acontece nos algoritmos de regressão Ridge ou Lasso, mas isso terá que ficar para outro tutorial. Por hora, a soma dos mínimos quadrados bastará como função custo, até porque ela tem a vantagem de deixar a matemática muito mais simples:\n",
    "\n",
    "$\\pmb{\\epsilon}^T  \\pmb{\\epsilon} = (\\pmb{y} - \\pmb{\\hat{w}}X)^T(\\pmb{y} - \\pmb{\\hat{w}} X) \\\\= \\pmb{y}^T \\pmb{y} - \\pmb{\\hat{w}}^T X^T \\pmb{y} - \\pmb{y}^T X \\pmb{\\hat{w}} + \\pmb{\\hat{w}} X^T X \\pmb{\\hat{w}} \\\\= \\pmb{y}^T \\pmb{y} - 2\\pmb{\\hat{w}}^T X^T \\pmb{y} + \\pmb{\\hat{w}} X^T X \\pmb{\\hat{w}}$\n",
    "\n",
    "![]()\n",
    "\n",
    "![]()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n9LoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1082dbc18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#read data\n",
    "dataframe = pd.read_fwf('brain_body.txt')\n",
    "x_values = dataframe[['Brain']]\n",
    "y_values = dataframe[['Body']]\n",
    "\n",
    "#train model on data\n",
    "body_reg = linear_model.LinearRegression()\n",
    "body_reg.fit(x_values, y_values)\n",
    "\n",
    "#visualize results\n",
    "plt.scatter(x_values, y_values)\n",
    "plt.plot(x_values, body_reg.predict(x_values))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para o modelo de Regressão Linear, será usada a classe do scikit-learn chamada **LinearRegression**. Essa classe tem a função **fit()** que treina o modelo nos seus dados.\n",
    "\n",
    "```\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "model.fit(x_values, y_values)\n",
    "```\n",
    "\n",
    "Como no exemplo acima, a variável model é um modelo de Regressão Linear que foi treinado nos dados de x_values e y_values. Treinar o modelo significa encontrar a melhor linha que se encaixe nos dados de treinamento. Vamos fazer duas predições usando a função predict() do modelo.\n",
    "\n",
    "```\n",
    "print(model.predict([ [127], [248] ]))\n",
    ">>> [[ 438.94308857, 127.14839521]]\n",
    "```\n",
    "\n",
    "O modelo retorna um vetor de previsões, uma previsão para cada vetor de entrada. O primeiro item, [127], teve a previsão de 438.94308857. Já o segundo item, [248], recebeu a previsão de 127.14839521. A razão para a previsão ser feita em um vetor como [127] e não apenas no número 127, é porque você pode ter um modelo que faz previsões baseadas em diversas características. Nós veremos o uso de múltiplas variáveis em regressão linear mais adiante nessa aula. Por enquanto, vamos estudar o uso de apenas uma.\n",
    "\n",
    "## Regressão Linear - Quiz\n",
    "Neste quiz, você trabalhará com dados sobre a média da expectativa de vida e a média do IMC no nascimento de homens ao redor do mundo. Os dados vem do Gapminder.\n",
    "\n",
    "O arquivo de dados pode ser encontrado na aba \"bmi_and_life_expectancy.csv\" do quiz abaixo. Os dados incluem também o país de origem das pessoas, presentes na coluna \"Country\". A expectativa de vida para uma pessoa daquele país está na coluna \"Life expectancy\". A média do IMC de uma criança nascida naquele país na coluna \"BMI\". Você preverá a expectativa de vida baseado no IMC.\n",
    "\n",
    "Você deverá completar cada um dos seguintes passos:\n",
    "\n",
    "#### 1. Carregar os dados\n",
    "\n",
    "* Os dados estão no arquivo chamado \"bmi_and_life_expectancy.csv\".\n",
    "* Use a função do pandas chamada read_csv para carregar os dados em um dataframe.\n",
    "* Salve o dataframe na variável bmi_life_data.\n",
    "\n",
    "#### 2. Construa um modelo de regressão linear\n",
    "\n",
    "* Crie um modelo de regressão usando o objeto LinearRegression do scikit-learn e salve-o na variável bmi_life_model.\n",
    "* Treine o modelo com os dados.\n",
    "\n",
    "#### 3. Faça previsões com o modelo\n",
    "* Faça previsões usando um IMC de 21.07931 e o salve na variável laos_life_exp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Add import statements\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Assign the dataframe to this variable.\n",
    "# TODO: Load the data\n",
    "bmi_life_data = pd.read_csv(\"bmi_and_life_expectancy.csv\")\n",
    "\n",
    "# Make and fit the linear regression model\n",
    "#TODO: Fit the model and Assign it to bmi_life_model\n",
    "bmi_life_model = LinearRegression()\n",
    "bmi_life_model.fit(bmi_life_data[['BMI']], bmi_life_data[['Life expectancy']])\n",
    "\n",
    "# Mak a prediction using the model\n",
    "# TODO: Predict life expectancy for a BMI value of 21.07931\n",
    "laos_life_exp = bmi_life_model.predict(21.07931)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 60.31564716]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "laos_life_exp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avisos sobre a regressão linear\n",
    "\n",
    "A regressão linear vem com uma série de suposições implícitas que nem sempre a fazem o melhor modelo para determinada situação. Eis alguns exemplos de problemas que você deve ter em mente.\n",
    "\n",
    "#### Regressão linear funciona melhor quando os dados são lineares\n",
    "A regressão linear produz um modelo que é uma linha reta extraída a partir dos dados de treinamento. Se a relação entre os dados de treinamento não for de fato linear, será necessário fazer alguns ajustes (transformações nos dados de treinamento), adicionar novas variáveis (veremos isso a seguir) ou, ainda, usar outro tipo de modelo\n",
    "\n",
    "![quadraticlinearregression](quadraticlinearregression.png)\n",
    "\n",
    "#### Regressões lineares são sensíveis a casos extraordinários\n",
    "A regressão linear tenta encontrar a linha que \"melhor se encaixa\" nos dados de treinamento. Caso o conjunto de dados tenha valores extremos que fujam muito do padrão geral, eles podem ter um efeito inesperadamente grande no modelo.\n",
    "\n",
    "Neste primeiro gráfico, o modelo se encaixa nos dados muito bem.\n",
    "\n",
    "![lin-reg-no-outliers](lin-reg-no-outliers.png)\n",
    "\n",
    "No entanto, se adicionarmos alguns pontos extraordinários que não se encaixam no padrão, o modo como o modelo faz previsões muda radicalmente.\n",
    "\n",
    "![lin-reg-w-outliers](lin-reg-w-outliers.png)\n",
    "\n",
    "Na maioria dos casos, será desejável criar um modelo que se encaixe na maioria dos dados, então, tome cuidado com os casos extraordinários!\n",
    "\n",
    "## Regressão linear múltipla\n",
    "Na última seção, vimos que é possível prever a expectativa de vida usando o IMC. Nesse caso, dizemos que o IMC era o preditor, também chamado de variável independente. O preditor é a variável que se procura para fazer previsões sobre outras variáveis, enquanto os valores que se busca prever são chamados de variável dependente. Neste nosso caso, a variável dependente era a expectativa de vida.\n",
    "\n",
    "Agora, digamos que também temos a frequência cardíaca de cada pessoa. É possível criar uma previsão de expectativa de vida usando tanto o IMC como a frequência cardíaca?\n",
    "\n",
    "Claro que sim! Podemos fazer exatamente isso usando a regressão linear múltipla.\n",
    "\n",
    "Caso o resultado que você quer prever dependa de mais de uma variável, é possível fazer um modelo um pouco mais complexo, que leve isso em consideração. Desde que sejam relevantes para situação, quanto mais variáveis preditoras/independentes seu modelo tiver, melhores podem ser as previsões.\n",
    "\n",
    "Quando há apenas um preditor, o modelo da regressão linear é uma linha, mas, ao adicionar mais variáveis preditoras, são adicionadas mais dimensões ao desenho.\n",
    "\n",
    "Ao usar a variável preditora, a equação da linha é:\n",
    "\n",
    "$y = mx + b$\n",
    "\n",
    "E o gráfico resultante é algo parecido com isso:\n",
    "\n",
    "![just-a-simple-lin-reg](just-a-simple-lin-reg.png)\n",
    "\n",
    "#### Regressão linear com uma variável preditora\n",
    "\n",
    "Ao adicionar uma variável preditora, totalizando duas, a equação de previsão vira:\n",
    "\n",
    "$y = m^1x^1 + m^2x^2 + b$\n",
    "\n",
    "Para representar isso graficamente, precisaremos de um gráfico tridimensional, com o modelo de regressão linear representado como um plano:\n",
    "\n",
    "![just-a-2d-reg](just-a-2d-reg.png)\n",
    "\n",
    "#### Regressão linear com duas variáveis preditoras\n",
    "\n",
    "Você pode usar mais de duas variáveis preditoras. Na verdade, pode usar quantos quiser. Se você utilizar nn variáveis preditoras, o modelo poderá ser representado pela equação\n",
    "\n",
    "$y = m^1x^1 + m^2x^2 + m^3x^3 + m^nx^n + b$\n",
    "\n",
    "Conforme constrói modelos com mais variáveis, a visualização vai ficando mais difícil, mas, por sorte, todo o restante em relação à regressão linear permanece igual. Ainda podemos incluir modelos e fazer previsões da mesma maneira - vamos fazer um teste!\n",
    "\n",
    "### Quiz de programação: Regressão linear múltipla\n",
    "Neste quiz, você usará o conjunto de dados [Boston house-prices](https://archive.ics.uci.edu/ml/datasets/Housing). Ele consiste em 13 características de 506 casas, e seu valor médio é de cerca de US$ 1000. Você aplicará um modelo às 13 características para prever o valor das casas.\n",
    "\n",
    "É necessário concluir os seguintes passos:\n",
    "\n",
    "#### 1. Construa um modelo de regressão linear\n",
    "* Crie um modelo de regressão linear usando o LinearRegression do scikit-learn e o aloque a model.\n",
    "* Aplique o modelo aos dados.\n",
    "\n",
    "#### 2. Faça previsões usando o modelo\n",
    "* Preveja o valor de sample_house."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "# Load the data from the the boston house-prices dataset \n",
    "boston_data = load_boston()\n",
    "x = boston_data['data']\n",
    "y = boston_data['target']\n",
    "\n",
    "# Make and fit the linear regression model\n",
    "# TODO: Fit the model and Assign it to the model variable\n",
    "model = LinearRegression()\n",
    "model.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Mak a prediction using the model\n",
    "sample_house = [[2.29690000e-01, 0.00000000e+00, 1.05900000e+01, 0.00000000e+00, 4.89000000e-01,\n",
    "                6.32600000e+00, 5.25000000e+01, 4.35490000e+00, 4.00000000e+00, 2.77000000e+02,\n",
    "                1.86000000e+01, 3.94870000e+02, 1.09700000e+01]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -1.07170557e-01,   4.63952195e-02,   2.08602395e-02,\n",
       "         2.68856140e+00,  -1.77957587e+01,   3.80475246e+00,\n",
       "         7.51061703e-04,  -1.47575880e+00,   3.05655038e-01,\n",
       "        -1.23293463e-02,  -9.53463555e-01,   9.39251272e-03,\n",
       "        -5.25466633e-01])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36.491103280359525"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 23.68420569])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Predict housing price for the sample_house\n",
    "prediction = model.predict(sample_house)\n",
    "\n",
    "# 23.68420569227329 is the correct prediction!\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Do Linear Regression the Right Way\n",
    "\n",
    "This is the code for the \"How to Do Linear Regression the Right Way\" live session by Siraj Raval on Youtube\n",
    "\n",
    "#### Overview\n",
    "This is the code for [this video on Youtube by Siraj Raval](https://youtu.be/uwwWVAgJBcM). I'm using a small dataset of student test scores and the amount of hours they studied. Intuitively, there must be a relationship right? The more you study, the better your test scores should be. We're going to use [linear regression](https://onlinecourses.science.psu.edu/stat501/node/250) to prove this relationship.\n",
    "\n",
    "Here are some helpful links:\n",
    "\n",
    "Gradient descent visualization\n",
    "![gd](https://raw.githubusercontent.com/mattnedrich/GradientDescentExample/master/gradient_descent_example.gif)\n",
    "\n",
    "Sum of squared distances formula (to calculate our error)\n",
    "![lre](https://spin.atomicobject.com/wp-content/uploads/linear_regression_error1.png)\n",
    "\n",
    "Partial derivative with respect to b and m (to perform gradient descent)\n",
    "![lrg](https://spin.atomicobject.com/wp-content/uploads/linear_regression_gradient1.png)\n",
    "\n",
    "* https://mathinsight.org/image/partial_derivative_as_slope\n",
    "* http://www.dummies.com/education/math/calculus/how-to-use-a-partial-derivative-to-measure-a-slope-in-three-dimensions/\n",
    "* https://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression/\n",
    "* https://www.quora.com/What-is-an-intuitive-explanation-of-gradient-descent\n",
    "* https://machinelearningmastery.com/gradient-descent-for-machine-learning/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The optimal values of m and b can be actually calculated with way less effort than doing a linear regression. \n",
    "#this is just to demonstrate gradient descent\n",
    "\n",
    "from numpy import *\n",
    "\n",
    "# y = mx + b\n",
    "# m is slope, b is y-intercept\n",
    "def compute_error_for_line_given_points(b, m, points):\n",
    "    # initialize it at 0\n",
    "    totalError = 0\n",
    "    # for every point\n",
    "    for i in range(0, len(points)):\n",
    "        # get the x value\n",
    "        x = points[i, 0]\n",
    "        # get the y value\n",
    "        y = points[i, 1]\n",
    "        # get the diference, square it, add it to the total\n",
    "        totalError += (y - (m * x + b)) ** 2\n",
    "        #print(\"error = \", totalError)\n",
    "        \n",
    "    # get the average\n",
    "    return totalError / float(len(points))\n",
    "\n",
    "# Magic the greatest the greatest\n",
    "def step_gradient(b_current, m_current, points, learningRate):\n",
    "    # starting points for our gradients\n",
    "    b_gradient = 0\n",
    "    m_gradient = 0\n",
    "    \n",
    "    # é a quantidade de pontos\n",
    "    N = float(len(points))\n",
    "    \n",
    "    # para cada ponto vamos calcular a derivada parcial em relação ao b e ao m.\n",
    "    # isso vai nos dar uma direção a seguir.\n",
    "    for i in range(0, len(points)):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        # direction with respect to b and m computing partial derivatives of our erro function\n",
    "        # esses são os calculos da nossa derivada parcial para b e m\n",
    "        b_gradient += -(2/N) * (y - ((m_current * x) + b_current))\n",
    "        m_gradient += -(2/N) * x * (y - ((m_current * x) + b_current))\n",
    "        \n",
    "    # update our b and m values using our partial derivatives    \n",
    "    new_b = b_current - (learningRate * b_gradient)\n",
    "    new_m = m_current - (learningRate * m_gradient)\n",
    "    return [new_b, new_m]\n",
    "\n",
    "def gradient_descent_runner(points, starting_b, starting_m, learning_rate, num_iterations):\n",
    "    # starting b and m\n",
    "    b = starting_b\n",
    "    m = starting_m\n",
    "    \n",
    "    # gradient descent\n",
    "    for i in range(num_iterations):\n",
    "        # update b and m with the new more accurate b and m by performing this gradient step\n",
    "        b, m = step_gradient(b, m, array(points), learning_rate)\n",
    "    return [b, m]\n",
    "\n",
    "def run():\n",
    "    # Step 1 - collect our data\n",
    "    points = genfromtxt(\"data.csv\", delimiter=\",\")\n",
    "    \n",
    "    # Step 2 - define our hyperparamters\n",
    "    # How fast should our model converge? Convergir significa obter o resultado ideal, o melhor ajuste da linha\n",
    "    learning_rate = 0.0001\n",
    "    # y = mx + b {Slope formula - Essa é a fórmula da inclinação}\n",
    "    # Todas as linhas seguem esta fórmula, onde m é a inclinação, b é a intercepção e x e y os pontos.\n",
    "    initial_b = 0 # initial y-intercept guess\n",
    "    initial_m = 0 # initial slope guess\n",
    "    num_iterations = 1000\n",
    "    \n",
    "    # Step 3 - train our model\n",
    "    # Exibe os valores iniciais de b e m.\n",
    "    print(\"Starting gradient descent at b = {0}, m = {1}, error = {2}\"\n",
    "          .format(initial_b, initial_m, \n",
    "                  compute_error_for_line_given_points(initial_b, initial_m, points)))\n",
    "    \n",
    "    print(\"\\nRunning...\")\n",
    "    \n",
    "    # Vamos executar nosso gradient descent que vai nos dar nossa inclinação (b) ideal, e nossa\n",
    "    # descida (m) ideal.\n",
    "    [b, m] = gradient_descent_runner(points, initial_b, initial_m, learning_rate, num_iterations)\n",
    "    \n",
    "    print(\"\\nAfter {0} iterations \\n  b = {1}, \\n  m = {2}, \\n  error = {3}\"\n",
    "          .format(num_iterations, b, m, \n",
    "                  compute_error_for_line_given_points(b, m, points)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent at b = 0, m = 0, error = 5565.107834483211\n",
      "\n",
      "Running...\n",
      "\n",
      "After 1000 iterations \n",
      "  b = 0.08893651993741346, \n",
      "  m = 1.4777440851894448, \n",
      "  error = 112.61481011613473\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
