{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão Linear\n",
    "\n",
    "Vamos supor que tenhamos dados em tabela sobre duas variáveis: **x** e **y**. Se colocarmos cada par **(x,y)** em um gráfico, teremos uma figura como a seguinte:\n",
    "\n",
    "![1](https://matheusfacure.github.io/img/tutorial/regr_lin_mqo/scatter.png)\n",
    "\n",
    "O que o algoritmo de regressão linear faz é simplesmente achar a reta que melhor se encaixa entre os pontos:\n",
    "\n",
    "Assim, podemos prever (com erro) um valor de y dado um valor de x. Por exemplo, nós não temos uma observação em que x=1, mas gostaríamos de prever qual seria o valor de y caso x fosse 1. Basta então olhar na linha qual valor de y quando x assume o valor 1. Na imagem acima, y seria aproximadamente 2.5 (ponto amarelo).\n",
    "\n",
    "![2](https://matheusfacure.github.io/img/tutorial/regr_lin_mqo/scatter_line.png)\n",
    "\n",
    "Ok. Esse exemplo é bem simples e meramente ilustrativo. Suponha agora que y não dependa mais apenas de x, mas de x e z. Bem, nesse caso, teríamos um gráfico em 3D e a regressão linear acharia o plano que melhor se encaixa nos dados. E para mais dimensões? Digamos que y dependa de 100 variáveis. Nós não podemos mais visualizar esse caso, mas sabemos que não é muito diferente dos casos 2D ou 3D, só que agora a regressão linear acha o hiperplano que melhor se encaixa nos dados. Se isso está um pouco abstrato e difícil de visualizar, pense  sempre em 3D quando trabalhando com muitas dimensões. É um truque muito útil que eu aprendi em um vídeo do professor de Geoffrey Hinton e, segundo ele, todo mundo faz isso: quando tentar visualizar 100 dimensões, por exemplo, pense em 3D e grite para si mesmo \"100D\" e você conseguirá abstrair grandes dimensionalidades.\n",
    "\n",
    "### Justificativa matemática\n",
    "Imagine que temos dados em tabela, sendo que cada linha é uma observação e cada coluna uma variável. Então escolhemos uma das colunas para ser nossa variável dependente y (aquela que queremos prever) e as outras serão as variáveis independentes (X). Nosso objetivo é aprender como chegar das variáveis independentes na variável dependente, ou, em outras palavras, prever y a partir de X. Note que X é uma matriz nxd, em que n é o número de observações e d o número de dimensões; y é um vetor coluna nx1. Podemos definir o problema como um sistema de equações em que cada equação é uma observação:\n",
    "\n",
    "$\\begin{cases}\n",
    "w_0 + w_1 x_1 + ... + w_d x_1 = y_1 \\\\\n",
    "w_0 + w_1 x_2 + ... + w_d x_2 = y_2 \\\\\n",
    "... \\\\\n",
    "w_0 + w_1 x_n + ... + w_d x_n = y_n \\\\\n",
    "\\end{cases}$\n",
    "\n",
    "Normalmente $n > d$, isto é, temos mais observações que dimensões. Sistemas assim costumam não ter solução, pois há muitas equações e poucas variáveis para ajustar. Intuitivamente, pense que, na prática, muitas coisas afetam a variável y, principalmente se ela for algo de interesse das ciências humanas como, por exemplo, preço, desemprego, felicidade etc. Muitas das coisas que afetam y não podem ser coletadas como dados; desse modo, as equações acima não teriam solução porque não teríamos todos os fatores que afetam y.\n",
    "\n",
    "Para lidar com esse problema, vamos adicionar nas equações um termo erro ε que representará os fatores que não conseguimos observar, erros de medição, etc.\n",
    "\n",
    "$\\begin{cases}\n",
    "w_0 + w_1 x_{11} + ... + w_d x_{1d} + \\varepsilon_1 = y_1 \\\\\n",
    "w_0 + w_1 x_{21} + ... + w_d x_{2d} + \\varepsilon_2 = y_2 \\\\\n",
    "... \\\\\n",
    "w_0 + w_1 x_{n1} + ... + w_d x_{nd} + \\varepsilon_3 = y_n \\\\\n",
    "\\end{cases}$\n",
    "\n",
    "Ou, em forma de matriz:\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "1 & x_{11} & ... & x_{1d} \\\\\n",
    "1 & x_{21} & ... & x_{2d} \\\\\n",
    "\\vdots & \\vdots& \\vdots & \\vdots \\\\\n",
    "1 & x_{n1} & ... & x_{nd} \\\\\n",
    "\\end{bmatrix}\n",
    "\\times\n",
    "\\begin{bmatrix}\n",
    "w_0 \\\\\n",
    "w_1 \\\\\n",
    "\\vdots \\\\\n",
    "w_d \\\\\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "\\varepsilon_0 \\\\\n",
    "\\varepsilon_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\varepsilon_n \\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "y_0 \\\\\n",
    "y_1 \\\\\n",
    "\\vdots \\\\\n",
    "y_n \\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$X_{nd} \\pmb{w}_{d1} + \\pmb{\\epsilon}_{n1} = \\pmb{y}_{n1}$\n",
    "\n",
    "![]()\n",
    "\n",
    "![]()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # para ler os dados em tabela\n",
    "import numpy as np # para álgebra linear\n",
    "from sklearn import linear_model, model_selection, datasets\n",
    "import matplotlib.pyplot as plt # para fazer gráficos\n",
    "from matplotlib import style\n",
    "from time import time # para ver quanto tempo demora\n",
    "np.random.seed(1) # para resultados consistentes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_regr(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        # adiciona coluna de 1 nos dados\n",
    "        X = np.insert(X_train, 0, 1, 1)\n",
    "\n",
    "        # estima os w_hat\n",
    "        # (X^T * X)^-1 * X^T * y\n",
    "        w_hat = np.dot(np.dot(np.linalg.inv(np.dot(X.T,X)),X.T),y_train)\n",
    "\n",
    "        self.w_hat = w_hat\n",
    "        self.coef = self.w_hat[1:]\n",
    "        self.intercept = self.w_hat[0]\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        X = np.insert(X_test, 0, 1, 1) # adiciona coluna de 1 nos dados\n",
    "        y_pred = np.dot(X, self.w_hat) # X * w_hat = y_hat\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#read data\n",
    "dataframe = pd.read_fwf('brain_body.txt')\n",
    "x_values = dataframe[['Brain']]\n",
    "y_values = dataframe[['Body']]\n",
    "\n",
    "#train model on data\n",
    "body_reg = linear_model.LinearRegression()\n",
    "body_reg.fit(x_values, y_values)\n",
    "\n",
    "#visualize results\n",
    "plt.scatter(x_values, y_values)\n",
    "plt.plot(x_values, body_reg.predict(x_values))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
